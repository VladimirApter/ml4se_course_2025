{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP9vt6bw/f18Z8v2SUEvBjW",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VladimirApter/ml4se_course_2025/blob/main/homeworks/ml4se_course_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Модель: ChatGPT 5**\n",
    "\n",
    "**Промпт:**\n",
    "Реализуй transformer Encoder с помощью PyTorch"
   ],
   "metadata": {
    "id": "B5fWlWo5frTk"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wFjgirsDfk4r",
    "ExecuteTime": {
     "end_time": "2025-10-28T09:09:32.090868Z",
     "start_time": "2025-10-28T09:09:32.073419Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# Positional Encoding\n",
    "# ---------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Scaled Dot-Product Attention\n",
    "# ---------------------------\n",
    "def attention(q, k, v):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, v), attn\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-Head Attention\n",
    "# ---------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv = nn.Linear(d_model, d_model * 3)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        def split_heads(t):\n",
    "            return t.view(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q, k, v = split_heads(q), split_heads(k), split_heads(v)\n",
    "        out, attn = attention(q, k, v)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, d_model)\n",
    "        return self.out(out), attn\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Feed-Forward Network\n",
    "# ---------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Encoder Layer\n",
    "# ---------------------------\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x + self.mha(x)[0])\n",
    "        x = self.norm2(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Encoder\n",
    "# ---------------------------\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, num_heads=4, num_layers=2, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    seq_len = 10\n",
    "    vocab_size = 50\n",
    "\n",
    "    x = torch.randint(0, vocab_size, (batch_size, seq_len))  # случайные токены для примера\n",
    "    encoder = TransformerEncoder(vocab_size)\n",
    "    out = encoder(x)\n",
    "    print(out)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.2748,  1.5467,  1.0017,  1.0257,  0.4898,  0.7739, -1.0106,\n",
      "          -0.7630, -1.3428,  0.9685, -0.4474,  0.6119, -0.6583,  0.2419,\n",
      "          -2.1787,  1.1806,  1.2875,  1.0062, -0.2527, -1.3480, -0.6815,\n",
      "          -0.0757, -0.9921, -0.1013, -0.2332,  1.8357, -0.3432, -0.2364,\n",
      "          -1.2520,  0.3052, -1.4978, -0.1353],\n",
      "         [ 0.9318,  0.0581, -1.1232,  0.8338,  0.7476, -0.3701, -0.8286,\n",
      "          -1.0097, -1.1500,  1.1058, -1.4011,  0.4856, -0.5064,  1.8823,\n",
      "          -1.1411, -0.4995, -0.9857,  1.1176,  0.3492, -0.6444, -0.2599,\n",
      "          -0.3085,  0.6084, -0.7561,  0.7005,  1.0891,  0.1033,  1.2254,\n",
      "          -2.3769,  1.9839, -0.0472,  0.1860],\n",
      "         [ 1.9675, -0.5258, -0.8209, -0.8997, -0.2780,  1.1369, -1.1226,\n",
      "           0.5430,  1.1129,  1.1498, -3.1179, -0.2546,  0.3334,  1.0349,\n",
      "          -1.5578, -0.1944,  0.6038, -0.7521, -0.2527, -0.9540,  0.5158,\n",
      "          -0.0922,  0.2762,  0.4092,  0.0435,  1.0747,  0.2018, -0.6792,\n",
      "          -0.5334, -0.6019,  0.7791,  1.4545],\n",
      "         [-0.3287, -1.8776,  0.8815, -0.3325,  0.8054,  1.0850, -1.2039,\n",
      "           0.8303, -0.2533,  1.2254, -0.4764,  0.4741,  0.2595, -1.2382,\n",
      "          -1.3805,  0.8124,  1.6164,  0.3451,  0.2301, -0.7690, -1.6319,\n",
      "           0.9435, -0.7530, -0.5356, -1.2654,  0.4580,  1.0895,  0.4669,\n",
      "          -1.0416,  1.6000, -1.1764,  1.1410],\n",
      "         [ 0.0971, -2.5493,  0.3823,  0.2284,  1.0620,  0.5594,  1.4658,\n",
      "           1.3485, -0.9143,  0.4951, -1.4541,  0.7480, -1.0720, -0.5748,\n",
      "          -0.0439, -0.6747,  0.1771, -1.9798, -0.3667, -0.2882, -1.0118,\n",
      "           0.6926, -0.2344,  0.4396, -1.2030,  0.4178,  1.3194,  0.1936,\n",
      "           1.0821,  0.1235, -0.3528,  1.8874],\n",
      "         [-0.4609,  0.5568,  0.0158, -1.0755,  2.4541, -0.1607,  0.9146,\n",
      "          -0.3542, -0.8858, -0.2031, -0.5244,  0.9157,  0.7593, -0.3657,\n",
      "          -2.9017,  0.4688, -0.3263, -0.1427,  0.3506,  1.1932, -0.0631,\n",
      "           1.2858,  0.9976,  0.0488,  0.4292, -0.1273, -0.4657, -1.5052,\n",
      "          -0.6111,  1.6445, -0.6148, -1.2466],\n",
      "         [ 0.3631, -1.1091, -0.8524, -0.2285,  1.2180, -0.1662,  1.8861,\n",
      "           1.2301, -0.7194,  0.4074, -1.1998,  0.7845, -1.2732, -0.7234,\n",
      "          -0.1052, -0.8394,  0.2144, -2.0206, -0.4470, -0.1508, -1.0986,\n",
      "           0.9428, -0.2437,  0.3799, -1.4429,  0.5963,  1.3796,  0.1098,\n",
      "           1.2154,  0.2300, -0.5402,  2.2030],\n",
      "         [ 1.0209, -1.4294, -1.4591, -0.2729,  0.8856,  0.5241, -0.4423,\n",
      "           0.2917, -0.9342,  0.4382,  0.4894,  0.3032, -0.4794, -0.2811,\n",
      "           0.2188,  0.4674,  1.5056,  1.8852, -1.2690, -0.5492, -0.5670,\n",
      "           1.1117, -1.4364, -1.9974,  0.6216,  1.8289,  1.1673, -0.7151,\n",
      "          -1.2948,  0.7381, -0.4697,  0.0991],\n",
      "         [ 1.2082, -0.2198, -1.0903, -0.4693,  0.3910, -1.2656,  0.3975,\n",
      "          -1.7101,  0.3512, -0.4783, -0.6350, -0.8578,  0.2339,  0.1468,\n",
      "          -2.8788,  1.0932, -0.7317,  1.3974,  1.0597,  0.0660, -0.0643,\n",
      "          -0.5889,  1.1055, -0.0681, -0.2828,  2.1863,  1.2781, -0.5343,\n",
      "           1.0513,  0.1442, -0.3051,  0.0700],\n",
      "         [ 0.1972, -2.0525, -1.0389, -0.1654,  1.3446,  0.8734,  0.8129,\n",
      "           0.0986, -0.6505, -0.2237,  0.2986,  0.0579, -0.9694, -0.0135,\n",
      "          -0.6470,  0.1173, -0.3414,  0.9010,  1.0581, -0.6143, -1.3075,\n",
      "           2.0068,  0.8066, -0.1638, -1.0638,  0.2412,  1.6171, -1.5978,\n",
      "          -1.6668,  1.7470,  0.6033, -0.2652]],\n",
      "\n",
      "        [[ 0.1834,  1.2650, -1.0893, -0.0739,  1.2609,  1.9295, -1.3769,\n",
      "          -0.3126, -0.0921, -0.1174, -1.4596,  0.9133,  1.0648,  0.3776,\n",
      "          -0.8168, -0.9366, -0.2492, -0.3508,  2.2732, -0.1262, -1.4000,\n",
      "          -0.0913, -2.1736,  0.0431, -0.4191, -0.1350,  0.1977, -0.2100,\n",
      "           0.2381, -0.7372,  0.9034,  1.5175],\n",
      "         [ 0.1588,  0.3178, -0.1521, -0.5490, -1.5930, -0.7986, -0.3036,\n",
      "           1.2147, -0.3800,  0.4381,  0.5735,  1.4809, -0.0117,  0.1107,\n",
      "          -1.4122, -0.0680, -0.9789,  0.0114, -0.4967, -1.2241, -0.6000,\n",
      "           1.4001,  1.3942, -1.2546, -0.7410,  0.8754, -0.3533, -0.3170,\n",
      "           1.0809,  1.6553, -1.7151,  2.2370],\n",
      "         [-0.2555, -2.2829, -0.6676,  1.1543, -0.6865,  0.3668, -0.3157,\n",
      "          -0.9323, -0.3911,  1.3077,  0.2092, -0.2250, -0.4958, -0.6084,\n",
      "          -1.1924,  0.0597,  0.7755,  1.3138, -0.2279, -0.3477, -0.1334,\n",
      "           0.6266, -1.3006,  0.6192, -1.0196,  1.3488,  2.1251, -1.6363,\n",
      "           0.1647,  0.8776, -0.1075,  1.8774],\n",
      "         [-0.0223, -2.5526,  3.0227, -0.6210,  0.3491,  1.0566,  0.7010,\n",
      "           0.1899, -0.3136, -0.6634, -0.8973,  0.6613, -1.5073, -0.1247,\n",
      "          -0.4205,  0.2322,  1.0178, -0.6359, -0.6592, -0.3152,  0.0336,\n",
      "          -0.2939,  0.7069,  1.1086, -0.1653,  0.2745, -1.5214,  0.0194,\n",
      "          -0.4034, -0.1841,  0.0604,  1.8675],\n",
      "         [ 0.3544, -0.8894, -1.0016, -1.8434,  0.3658,  0.8690, -0.8280,\n",
      "           0.7589,  1.3662,  0.8884, -3.0603, -0.3082,  0.9701,  0.9642,\n",
      "          -1.4148, -0.0837,  0.5886, -0.4989, -0.3111, -0.4865,  0.6448,\n",
      "           0.0190,  0.3742,  0.5861, -0.0492,  1.2845,  0.0210, -0.7519,\n",
      "          -0.4469, -0.7466,  1.0321,  1.6331],\n",
      "         [-1.2419, -0.3974,  0.3579, -0.6713,  0.5305, -1.4711,  1.8502,\n",
      "          -1.0535,  0.4048, -0.9638, -0.4453, -1.1864,  0.6321,  1.0232,\n",
      "          -1.0115,  0.9990, -0.0204, -0.1167,  1.3345, -0.7075,  0.5437,\n",
      "           0.7533, -1.1145,  0.3375, -1.2891,  1.2429, -0.8072, -0.2914,\n",
      "          -0.1723,  2.3517,  1.3904, -0.7905],\n",
      "         [-0.7261,  0.8090,  0.6118, -2.2944,  1.0438, -0.9471, -0.3619,\n",
      "           1.3199,  0.8865,  0.9668, -0.4251, -1.0240,  0.0291, -0.0477,\n",
      "          -0.0058,  1.6098,  0.2531,  1.1974, -1.6116,  0.6023, -0.3222,\n",
      "          -0.1782, -1.1127,  0.6685, -1.5966,  0.4872,  0.9458, -1.1590,\n",
      "          -1.1251,  0.8840, -0.7448,  1.3675],\n",
      "         [-0.9514,  1.3405, -1.7663, -0.8360,  1.7171,  0.0193,  1.0117,\n",
      "           0.9631, -1.7577,  0.0131,  1.0472, -0.7815,  0.1233, -0.0325,\n",
      "          -0.9993, -0.0577,  0.6878,  0.2124, -0.0801,  0.0886, -0.1056,\n",
      "           1.0463,  0.7051,  0.1752, -0.8697,  0.1104,  0.7578, -2.6711,\n",
      "          -1.0192,  0.7685, -0.3889,  1.5295],\n",
      "         [ 0.9400, -0.8579, -1.0281,  0.4396, -1.3883, -1.1464, -1.0628,\n",
      "           1.1717, -0.5706, -0.1757, -0.2935, -1.4682, -1.0152, -0.5514,\n",
      "          -1.9926,  0.7616,  0.6356,  0.1410,  0.4806,  1.3694, -0.2007,\n",
      "           0.6034,  1.0046,  1.2578,  0.3700,  1.7268,  0.7175, -0.3670,\n",
      "          -0.9646,  2.0349, -0.7183,  0.1467],\n",
      "         [ 1.4997, -2.6849, -0.7159, -1.0225, -0.9050, -0.5944,  0.3465,\n",
      "           0.0505,  0.6792, -0.6206, -0.2535, -0.4046, -0.5450, -0.0614,\n",
      "          -0.4246, -0.3371,  0.9566,  1.4514, -0.0264, -0.4766, -1.2630,\n",
      "          -0.6656,  0.9176,  2.0216, -0.9803,  0.8461,  0.9835,  0.9267,\n",
      "           0.9164, -0.0435, -1.0828,  1.5119]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ]
}
