{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyP9vt6bw/f18Z8v2SUEvBjW",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VladimirApter/ml4se_course_2025/blob/main/homeworks/ml4se_course_hw3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Модель: ChatGPT 5**\n",
    "\n",
    "**Промпт:**\n",
    "Нужно решить задачу токенезации строкового представления кода.\n",
    "\n",
    "На вход нам дается строковое представление фрагмента кода на Python (3.11). Этот код нужно превратить в последовательность целых чисел, то есть представить его в виде последовательности токенов. Для токенезации используй BPE и следующий ряд правил:\n",
    "1. Каждое ключевое слово Python должно целиком превратиться в отдельный токен.\n",
    "2. Все переносы строк должны превратиться в отдельный токен.\n",
    "3. Все значимые пробельные символы должны целиком превращаться в отдельный токен. Значимыми пробельными символами являются пробелы и символы табуляции, которые предназначены для обозначения вложенности в Python.\n",
    "\n",
    "Напиши программу на Python которая будет обрабатывать строковое представление кода вышеописанным способом.\n"
   ],
   "metadata": {
    "id": "B5fWlWo5frTk"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wFjgirsDfk4r",
    "ExecuteTime": {
     "end_time": "2025-10-15T05:23:42.270603Z",
     "start_time": "2025-10-15T05:23:42.260264Z"
    }
   },
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {}\n",
    "        self.inverse_vocab = {}\n",
    "        self.next_token_id = 256\n",
    "\n",
    "        for i in range(256):\n",
    "            self.vocab[chr(i)] = i\n",
    "            self.inverse_vocab[i] = chr(i)\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_python_tokens(text):\n",
    "        patterns = [\n",
    "            (r'\\n', 'NEWLINE'),\n",
    "            (r'[ \\t]+', 'INDENT'),\n",
    "            (r'\\b(and|as|assert|async|await|break|class|continue|def|del|elif|else|except|finally|for|from|global|if|import|in|is|lambda|nonlocal|not|or|pass|raise|return|try|while|with|yield|True|False|None)\\b', 'KEYWORD'),\n",
    "        ]\n",
    "\n",
    "        tokens = []\n",
    "        pos = 0\n",
    "        text_len = len(text)\n",
    "\n",
    "        while pos < text_len:\n",
    "            matched = False\n",
    "            for pattern, token_type in patterns:\n",
    "                regex = re.compile(pattern)\n",
    "                match = regex.match(text, pos)\n",
    "                if match:\n",
    "                    token = match.group()\n",
    "                    if token_type in ['KEYWORD', 'NEWLINE', 'INDENT']:\n",
    "                        tokens.append((token, token_type))\n",
    "                    else:\n",
    "                        tokens.extend([(ch, 'CHAR') for ch in token])\n",
    "\n",
    "                    pos = match.end()\n",
    "                    matched = True\n",
    "                    break\n",
    "\n",
    "            if not matched:\n",
    "                tokens.append((text[pos], 'CHAR'))\n",
    "                pos += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_stats(tokens):\n",
    "        pairs = defaultdict(int)\n",
    "        for i in range(len(tokens) - 1):\n",
    "            if tokens[i][1] == 'CHAR' and tokens[i+1][1] == 'CHAR':\n",
    "                pairs[(tokens[i][0], tokens[i+1][0])] += 1\n",
    "        return pairs\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_pair(tokens, pair):\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if (i < len(tokens) - 1 and\n",
    "                tokens[i][1] == 'CHAR' and tokens[i+1][1] == 'CHAR' and\n",
    "                tokens[i][0] == pair[0] and tokens[i+1][0] == pair[1]):\n",
    "                new_token = pair[0] + pair[1]\n",
    "                new_tokens.append((new_token, 'CHAR'))\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "\n",
    "    def train(self, text, num_merges=100):\n",
    "        tokens = self._extract_python_tokens(text)\n",
    "\n",
    "        for _ in range(num_merges):\n",
    "            pairs = self._get_stats(tokens)\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            best_pair = max(pairs.items(), key=lambda x: x[1])[0]\n",
    "\n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            self.vocab[new_token] = self.next_token_id\n",
    "            self.inverse_vocab[self.next_token_id] = new_token\n",
    "            self.next_token_id += 1\n",
    "\n",
    "            tokens = self._merge_pair(tokens, best_pair)\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        tokens = self._extract_python_tokens(text)\n",
    "\n",
    "        changed = True\n",
    "        while changed:\n",
    "            changed = False\n",
    "            for token_str, token_id in sorted(self.vocab.items(),\n",
    "                                            key=lambda x: len(x[0]),\n",
    "                                            reverse=True):\n",
    "                if len(token_str) > 1:\n",
    "                    new_tokens = []\n",
    "                    i = 0\n",
    "                    while i < len(tokens):\n",
    "                        if (tokens[i][1] == 'CHAR' and\n",
    "                            i + len(token_str) - 1 < len(tokens) and\n",
    "                            all(tokens[i+j][1] == 'CHAR' for j in range(len(token_str))) and\n",
    "                            ''.join(tokens[i+j][0] for j in range(len(token_str))) == token_str):\n",
    "\n",
    "                            new_tokens.append((token_str, 'CHAR'))\n",
    "                            i += len(token_str)\n",
    "                            changed = True\n",
    "                        else:\n",
    "                            new_tokens.append(tokens[i])\n",
    "                            i += 1\n",
    "                    tokens = new_tokens\n",
    "\n",
    "        token_ids = []\n",
    "        for token, token_type in tokens:\n",
    "            if token_type in ['KEYWORD', 'NEWLINE', 'INDENT']:\n",
    "                if token not in self.vocab:\n",
    "                    self.vocab[token] = self.next_token_id\n",
    "                    self.inverse_vocab[self.next_token_id] = token\n",
    "                    self.next_token_id += 1\n",
    "                token_ids.append(self.vocab[token])\n",
    "            elif token in self.vocab:\n",
    "                token_ids.append(self.vocab[token])\n",
    "            else:\n",
    "                for char in token:\n",
    "                    token_ids.append(self.vocab[char])\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    python_code_example = \"\"\"\n",
    "def factorial(n):\n",
    "    if n <= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return n * factorial(n-1)\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "\n",
    "    def add(self, x):\n",
    "        self.result += x\n",
    "        return self.result\n",
    "\"\"\"\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.train(python_code_example, num_merges=50)\n",
    "\n",
    "    tokens = tokenizer.tokenize(python_code_example)\n",
    "\n",
    "    print(\"Исходный код:\")\n",
    "    print(python_code_example)\n",
    "    print(\"\\nТокены:\")\n",
    "    print(tokens)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный код:\n",
      "\n",
      "def factorial(n):\n",
      "    if n <= 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial(n-1)\n",
      "\n",
      "class Calculator:\n",
      "    def __init__(self):\n",
      "        self.result = 0\n",
      "\n",
      "    def add(self, x):\n",
      "        self.result += x\n",
      "        return self.result\n",
      "\n",
      "\n",
      "Токены:\n",
      "[10, 304, 32, 279, 10, 305, 306, 32, 110, 32, 280, 32, 281, 10, 307, 308, 32, 49, 10, 305, 309, 58, 10, 307, 308, 32, 110, 32, 42, 32, 284, 10, 10, 310, 32, 290, 10, 305, 304, 32, 297, 10, 307, 269, 32, 61, 32, 48, 10, 10, 305, 304, 32, 301, 32, 302, 10, 307, 269, 32, 303, 32, 120, 10, 307, 308, 32, 269, 10]\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ]
}
