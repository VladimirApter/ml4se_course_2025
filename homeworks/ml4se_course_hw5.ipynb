{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/VladimirApter/ml4se_course_2025/blob/main/homeworks/ml4se_course_hw5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Модель: ChatGPT 5**\n",
    "\n",
    "**Промпт:**\n",
    "Реализуй transformer Decoder с помощью PyTorch"
   ],
   "metadata": {
    "id": "B5fWlWo5frTk"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wFjgirsDfk4r",
    "ExecuteTime": {
     "end_time": "2025-11-04T09:09:59.175464Z",
     "start_time": "2025-11-04T09:09:59.155412Z"
    }
   },
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ---------------------------\n",
    "# Positional Encoding\n",
    "# ---------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Scaled Dot-Product Attention\n",
    "# ---------------------------\n",
    "def attention(q, k, v, mask=None):\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(q.size(-1))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    attn = F.softmax(scores, dim=-1)\n",
    "    return torch.matmul(attn, v), attn\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Multi-Head Attention\n",
    "# ---------------------------\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q_proj = nn.Linear(d_model, d_model)\n",
    "        self.k_proj = nn.Linear(d_model, d_model)\n",
    "        self.v_proj = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q_input, kv_input, mask=None):\n",
    "        batch, q_len, d_model = q_input.size()\n",
    "        k_len = kv_input.size(1)\n",
    "\n",
    "        q = self.q_proj(q_input)\n",
    "        k = self.k_proj(kv_input)\n",
    "        v = self.v_proj(kv_input)\n",
    "\n",
    "        def split_heads(x):\n",
    "            return x.view(batch, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q, k, v = split_heads(q), split_heads(k), split_heads(v)\n",
    "\n",
    "        out, attn = attention(q, k, v, mask)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, q_len, d_model)\n",
    "        return self.out(out), attn\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Feed-Forward Network\n",
    "# ---------------------------\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Decoder Layer\n",
    "# ---------------------------\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = FeedForward(d_model, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, enc_out, tgt_mask=None):\n",
    "        x = self.norm1(x + self.self_attn(x, x, mask=tgt_mask)[0])\n",
    "        x = self.norm2(x + self.cross_attn(x, enc_out)[0])\n",
    "        x = self.norm3(x + self.ffn(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Transformer Decoder\n",
    "# ---------------------------\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=32, num_heads=4, num_layers=2, max_len=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads) for _ in range(num_layers)])\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, tgt, enc_out):\n",
    "        x = self.embedding(tgt)\n",
    "        x = self.pos_encoding(x)\n",
    "\n",
    "        seq_len = tgt.size(1)\n",
    "        tgt_mask = torch.tril(torch.ones(seq_len, seq_len)).to(tgt.device)\n",
    "        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, tgt_mask=tgt_mask)\n",
    "\n",
    "        return self.output(x)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Пример использования без энкодера\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    seq_len_src = 10\n",
    "    seq_len_tgt = 8\n",
    "    vocab_size = 50\n",
    "    d_model = 32\n",
    "\n",
    "    # фиктивный выход энкодера\n",
    "    enc_out = torch.randn(batch_size, seq_len_src, d_model)\n",
    "    tgt = torch.randint(0, vocab_size, (batch_size, seq_len_tgt))\n",
    "\n",
    "    decoder = TransformerDecoder(vocab_size, d_model=d_model)\n",
    "    out = decoder(tgt, enc_out)\n",
    "\n",
    "    print(out)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.7912e-01,  2.5063e-01, -3.7944e-01, -9.9683e-01, -5.7249e-01,\n",
      "          -5.7281e-01, -2.5439e-01, -1.8484e-01, -4.5297e-01,  2.8060e-01,\n",
      "           3.0251e-02,  1.2683e+00,  4.7186e-01, -1.4607e-02,  5.0784e-01,\n",
      "           5.1652e-03, -1.1180e+00,  1.7665e+00, -7.8176e-02, -1.6068e-01,\n",
      "           3.5998e-01,  2.4024e-01,  1.4772e-01,  5.0254e-01, -9.8214e-02,\n",
      "           1.9857e-01, -2.2004e-01, -3.5355e-01, -6.4590e-01, -7.4408e-01,\n",
      "          -5.7827e-02, -8.7296e-01,  1.9951e-01,  6.8790e-01,  5.6589e-01,\n",
      "           1.7189e-01,  7.3011e-01, -2.5535e-01, -7.9195e-02,  1.5903e-01,\n",
      "          -3.9241e-01,  2.9310e-01,  3.4093e-01, -6.3920e-01,  6.3215e-02,\n",
      "           1.5241e-01,  8.0693e-01, -7.2311e-02,  1.5255e+00,  2.4122e-01],\n",
      "         [ 5.9022e-02,  1.4557e-01, -1.1977e+00, -1.6888e-01, -7.5865e-01,\n",
      "           2.0378e-01,  5.3450e-02, -8.9729e-01, -9.1389e-02,  5.1869e-01,\n",
      "          -5.2297e-01,  9.9579e-01, -3.1255e-02,  1.2901e-01,  2.1577e-01,\n",
      "           1.3955e-01, -1.1218e+00,  9.4921e-01,  9.1609e-01, -1.9310e-01,\n",
      "           6.6003e-02,  1.6899e-01, -1.2681e+00, -2.2177e-01,  3.8970e-01,\n",
      "           6.5178e-01,  8.5637e-01,  2.2902e-01, -5.9438e-01, -1.2674e+00,\n",
      "           7.8673e-02, -1.4855e+00, -2.7160e-01,  6.6242e-01,  9.4330e-01,\n",
      "          -3.1794e-01,  6.4430e-01, -8.5959e-01,  2.6698e-01,  8.2170e-01,\n",
      "          -2.8874e-01,  7.5353e-01,  4.5051e-01, -5.2444e-01, -1.5091e-01,\n",
      "           4.0584e-01,  8.4319e-01, -6.1279e-01,  1.6223e+00,  2.3822e-01],\n",
      "         [ 8.9335e-02,  4.0382e-01, -7.7562e-01,  6.7131e-01,  8.1920e-01,\n",
      "          -8.5558e-01,  1.3436e+00, -3.0725e-01,  9.8920e-01,  1.2367e+00,\n",
      "           6.2029e-01, -2.8905e-01,  3.6455e-01,  2.9455e-01, -9.7925e-01,\n",
      "          -1.0710e+00, -1.1764e+00,  7.9547e-01,  2.3161e-02, -5.4939e-01,\n",
      "           1.5430e-01,  1.1378e+00, -1.6251e-01, -4.7224e-01,  2.6698e-02,\n",
      "           6.6437e-01, -3.5471e-01, -3.5081e-01, -1.3392e-01, -1.2347e+00,\n",
      "          -1.7443e-02, -1.0380e+00, -1.9935e-01, -6.4457e-02,  6.7929e-01,\n",
      "           6.7820e-01, -4.3764e-01, -1.0326e+00,  7.7954e-01,  4.6371e-02,\n",
      "          -1.1566e+00,  4.0432e-01,  2.8836e-01, -2.3552e-02, -1.8893e-01,\n",
      "           7.3933e-01, -2.9550e-01, -4.7142e-01,  1.4489e+00, -5.7525e-01],\n",
      "         [-2.6887e-01,  9.3323e-02,  6.2148e-01,  1.7853e-01, -7.7943e-01,\n",
      "          -4.1477e-02,  9.1301e-02, -8.7863e-02, -3.9144e-01, -1.0089e+00,\n",
      "           3.5669e-01,  2.2927e-01, -4.4653e-01,  7.2059e-02, -1.7867e-01,\n",
      "          -4.1196e-01, -6.4846e-02,  5.0645e-01, -1.8111e-02, -8.3769e-02,\n",
      "          -9.2919e-01, -5.2717e-01, -3.7248e-01,  9.9664e-01, -4.0663e-01,\n",
      "           4.3198e-01,  2.3224e-01, -1.0167e+00, -4.1988e-01, -6.8720e-01,\n",
      "           6.1816e-01,  2.9308e-01,  8.6053e-01, -2.7094e-01, -4.6521e-02,\n",
      "           5.4070e-01,  3.9210e-01,  1.9123e-01,  4.6917e-01,  4.6265e-01,\n",
      "           6.1283e-01, -3.3820e-02,  1.0161e-01, -8.9169e-01, -3.0742e-01,\n",
      "          -5.3854e-01, -1.4891e-01,  2.7488e-01,  8.8992e-01, -6.0106e-01],\n",
      "         [ 4.4405e-01, -3.5189e-01,  2.7853e-01, -3.5380e-01,  1.9996e-01,\n",
      "          -5.9735e-01, -4.2537e-01, -5.8789e-01, -6.7081e-01,  3.4527e-01,\n",
      "           1.4849e-01,  3.5413e-01, -4.8466e-01, -2.7510e-01, -5.4383e-01,\n",
      "          -8.2220e-01, -6.1822e-02,  3.3240e-03,  2.0522e-01, -1.8006e-01,\n",
      "          -6.6534e-01,  1.6543e-02, -1.6446e-01,  4.2079e-01, -5.1277e-01,\n",
      "           6.3660e-01, -2.0111e-01, -1.1238e+00, -5.2527e-01, -1.2268e+00,\n",
      "           6.0639e-01, -4.9161e-01, -3.2142e-02, -1.1072e+00, -4.2157e-02,\n",
      "          -1.4955e-01, -2.7948e-01,  3.6708e-02,  6.4074e-01, -4.2409e-01,\n",
      "           9.7279e-01, -5.7673e-01,  9.7746e-01, -4.6580e-01, -1.3249e+00,\n",
      "           7.4179e-01, -1.0155e-01,  5.2679e-01, -2.0005e-01, -6.8035e-01],\n",
      "         [-2.5340e-01, -3.5166e-01,  7.5246e-01, -3.2516e-01,  4.9061e-01,\n",
      "          -3.4089e-01, -4.1078e-02, -5.7124e-01, -8.9427e-01, -1.0050e-01,\n",
      "          -8.3478e-01,  3.4337e-01, -7.3694e-01,  8.1768e-01,  1.1260e-01,\n",
      "          -4.9980e-01, -1.8287e-01, -3.4621e-01,  3.0115e-01,  6.4514e-01,\n",
      "          -2.3509e-01,  2.3890e-02, -6.4969e-01,  3.3327e-01, -7.6142e-01,\n",
      "           1.9441e-01,  2.8284e-02, -8.6850e-01,  9.2272e-02, -3.7657e-01,\n",
      "           2.9085e-01,  1.2182e+00,  6.0144e-01, -4.7447e-01, -2.1811e-01,\n",
      "           4.7900e-01,  4.5642e-01, -7.3690e-01,  1.8259e-01,  4.7579e-01,\n",
      "           2.4873e-01,  7.6401e-02, -3.1691e-01,  3.9073e-01, -6.0074e-01,\n",
      "           5.4393e-01,  7.3528e-01, -3.0002e-01, -1.5489e-01, -1.2468e-02],\n",
      "         [ 8.2953e-02, -5.4392e-01,  1.6321e+00, -4.3206e-01,  4.3174e-01,\n",
      "          -4.9292e-01,  1.1401e-01, -7.0850e-01, -1.1374e+00, -1.6905e-01,\n",
      "          -3.6340e-01, -5.8722e-01, -4.9366e-01, -1.2187e-01, -6.2116e-01,\n",
      "           9.7001e-02, -7.2685e-01, -5.5991e-01,  5.9778e-01, -5.0622e-01,\n",
      "           1.4202e-02, -3.1327e-01,  5.2283e-01,  4.5594e-01, -4.5123e-01,\n",
      "           2.8649e-01, -6.3116e-02, -1.0166e+00, -1.4410e-01, -3.8058e-01,\n",
      "          -1.4830e-02,  5.7850e-01, -2.9685e-01, -2.5160e-01,  3.8714e-01,\n",
      "           1.4468e-02, -3.7389e-01, -8.4628e-01,  1.0022e+00,  4.6707e-01,\n",
      "          -2.2575e-01, -8.9164e-01,  1.1300e+00, -5.9364e-01, -7.4956e-01,\n",
      "          -1.2696e-01,  2.0492e-02, -1.4911e+00,  9.1197e-01, -4.4846e-01],\n",
      "         [-1.0303e-01,  3.0361e-01, -5.4864e-02,  1.2478e-01,  4.3682e-01,\n",
      "          -7.0783e-01,  1.0249e+00,  1.7251e-01, -1.4133e-01,  9.3826e-01,\n",
      "           2.9496e-01,  2.1434e-01, -5.4730e-01, -7.1037e-01, -8.3224e-01,\n",
      "          -6.8798e-01, -1.3171e+00,  3.8429e-01,  2.4711e-02, -6.2947e-01,\n",
      "           7.0967e-01,  4.3642e-01, -4.9411e-02,  2.0300e-03, -3.1094e-01,\n",
      "           8.5850e-01, -3.7281e-01, -1.1106e-01, -2.2901e-01, -6.6746e-01,\n",
      "           1.7459e-01, -1.5572e-01, -5.0201e-01, -2.5821e-01,  1.3181e-01,\n",
      "           6.8372e-02, -5.2076e-01, -1.0503e+00,  1.1221e+00,  3.2136e-01,\n",
      "          -9.2455e-01, -3.0890e-01,  7.0318e-01, -8.0973e-01, -6.0660e-01,\n",
      "           3.3415e-01,  1.0149e+00, -1.2864e+00,  1.2427e+00, -3.4481e-01]],\n",
      "\n",
      "        [[ 7.9759e-01,  4.6253e-01, -5.7521e-01, -5.3741e-01,  3.5884e-01,\n",
      "          -3.3999e-01, -5.4341e-01, -7.3737e-01, -6.2772e-01,  4.6018e-01,\n",
      "          -3.5173e-01,  8.1838e-01, -1.9304e-01, -1.6346e-02, -5.6344e-01,\n",
      "          -6.2618e-01, -6.6213e-01,  6.6331e-01,  6.8062e-01, -2.9583e-01,\n",
      "          -6.5578e-01,  7.8919e-02, -3.5846e-02, -4.6203e-03, -3.8651e-01,\n",
      "           5.1533e-01,  2.2939e-01, -4.7815e-01, -4.7607e-01, -1.0544e+00,\n",
      "           7.7727e-01, -8.8400e-01,  2.4965e-01, -3.0335e-01,  3.2557e-01,\n",
      "          -2.6710e-01, -1.7797e-01, -3.1646e-01,  1.6740e-01,  7.6781e-02,\n",
      "           7.6613e-01, -5.8696e-01,  1.0901e+00, -9.0424e-02, -1.5676e+00,\n",
      "           2.7259e-01, -1.2405e-01,  3.7841e-01,  1.9187e-01, -7.3420e-01],\n",
      "         [-3.4999e-01,  7.5612e-01, -1.1778e+00,  5.0366e-01,  3.1326e-01,\n",
      "          -3.0602e-01,  4.3650e-01,  4.7433e-01,  1.1742e+00,  5.3927e-01,\n",
      "           5.0776e-01, -2.7822e-01,  8.9193e-01,  7.5959e-01, -4.8558e-01,\n",
      "          -1.2744e+00, -3.8465e-01,  4.1218e-01, -1.0934e-01, -1.7387e-01,\n",
      "          -6.9066e-01,  9.7235e-02, -4.0135e-01, -3.6360e-01,  5.3126e-01,\n",
      "           5.1770e-01, -3.4282e-01, -7.0971e-01, -8.8570e-01, -1.4804e-01,\n",
      "           1.7275e-01, -7.8974e-01,  9.7872e-01,  9.8404e-02,  9.2509e-01,\n",
      "           8.1955e-01,  6.3229e-01, -6.5455e-01,  1.2824e-01,  4.1879e-02,\n",
      "          -2.5405e-01,  5.3993e-02, -2.4240e-01,  5.9895e-01,  3.4963e-01,\n",
      "          -2.3630e-01, -9.8977e-01,  2.9446e-01,  7.1701e-01, -2.1265e-01],\n",
      "         [ 1.0423e-01,  7.2425e-01, -8.6743e-01,  5.1861e-02,  2.0312e-01,\n",
      "          -2.7498e-01,  6.7894e-01, -4.1221e-01,  6.5052e-01, -2.0256e-01,\n",
      "          -7.1749e-01, -6.4951e-01, -5.5521e-01,  9.7681e-01, -6.8369e-01,\n",
      "          -8.5552e-02, -1.2216e+00,  3.7518e-02,  3.7883e-01, -2.5395e-01,\n",
      "          -6.5745e-01, -1.0481e-01, -8.0906e-01, -8.0406e-02,  7.9529e-01,\n",
      "           9.1377e-01,  1.3771e-02, -8.3873e-01, -1.0671e+00, -5.3098e-01,\n",
      "           5.4019e-01, -2.5344e-01,  1.7707e-01,  3.8471e-01,  2.7606e-01,\n",
      "          -1.9507e-02,  5.1777e-01, -8.5669e-01,  1.1034e+00,  5.4746e-01,\n",
      "           6.2598e-02, -3.2824e-01,  3.5188e-01,  6.3229e-01, -7.8058e-01,\n",
      "          -7.0520e-02,  3.3109e-01,  2.3287e-01,  5.3614e-01, -7.5418e-02],\n",
      "         [-1.8025e-01,  1.5715e-01, -3.4557e-01,  6.0789e-01,  2.5030e-01,\n",
      "          -3.9405e-01,  6.0851e-01, -2.4049e-01,  2.4303e-01,  3.2075e-01,\n",
      "           6.3370e-01,  5.7059e-01,  7.8330e-01, -1.0084e-03,  1.2410e-01,\n",
      "          -7.8549e-01, -6.6142e-01,  3.3044e-01, -5.4512e-01, -9.0861e-02,\n",
      "          -2.7022e-01,  2.6532e-01, -1.3614e-02,  4.0784e-01, -1.8586e-01,\n",
      "           8.2887e-01, -3.0396e-01, -8.5292e-01, -6.4872e-01, -5.3607e-01,\n",
      "           5.5973e-01, -4.6357e-01,  5.6980e-01, -2.3643e-01,  1.2626e-01,\n",
      "           9.7093e-01, -1.4584e-01, -9.7584e-01,  3.6840e-01, -3.0104e-01,\n",
      "          -1.9909e-01, -2.7354e-01, -3.7208e-02, -2.4072e-01, -3.0827e-01,\n",
      "           1.9890e-01, -1.8720e-01,  5.6816e-01,  5.9795e-01, -5.3380e-01],\n",
      "         [-3.2831e-01,  2.1635e-01, -1.1344e-01, -6.8658e-01, -3.0362e-01,\n",
      "          -7.7511e-01, -2.5512e-01, -1.2687e-01, -4.0808e-01,  9.6152e-02,\n",
      "           3.0584e-01,  9.8482e-01, -3.2470e-01,  3.6571e-03, -5.7257e-02,\n",
      "          -6.1531e-01, -9.5463e-01,  1.4004e+00,  8.3827e-02, -3.2862e-01,\n",
      "           3.5952e-01, -3.1026e-02, -1.1678e-01,  6.9147e-01, -3.6514e-01,\n",
      "           2.6104e-01, -3.8872e-01, -8.9077e-01, -4.3775e-01, -1.2065e+00,\n",
      "           2.4923e-01, -6.8573e-01,  3.4326e-01,  1.3985e-02,  7.5085e-02,\n",
      "           4.7364e-01,  4.9654e-01,  2.8804e-02,  2.9871e-01, -2.4788e-02,\n",
      "          -3.1433e-01,  2.3888e-01,  4.7382e-01, -1.1669e+00, -1.3372e-01,\n",
      "           7.3386e-01,  7.5692e-01,  8.3733e-02,  1.2492e+00, -4.0538e-02],\n",
      "         [ 1.1692e-01, -2.8786e-01,  6.4734e-01,  6.9816e-02, -2.2385e-01,\n",
      "          -2.7364e-01, -9.2774e-02, -1.6362e-01, -6.5758e-01, -7.0236e-01,\n",
      "           1.7668e-01,  1.9977e-02, -7.6421e-01, -2.1598e-01, -7.7418e-01,\n",
      "          -6.5690e-01, -6.7994e-02,  2.7728e-01,  5.0882e-01, -8.1090e-02,\n",
      "          -1.1771e+00, -4.7647e-01, -3.2364e-01,  7.0699e-01, -5.3550e-01,\n",
      "           1.8704e-01,  1.2638e-01, -1.0708e+00, -3.4479e-01, -8.5401e-01,\n",
      "           5.0044e-01,  1.6152e-01,  5.8747e-01, -3.0718e-01, -1.9495e-01,\n",
      "           4.3974e-01,  4.9624e-02,  2.1439e-02,  5.0899e-01,  4.2345e-01,\n",
      "           6.7382e-01, -4.0307e-01,  2.6210e-01, -9.6080e-01, -6.1994e-01,\n",
      "          -4.4632e-01, -1.5254e-01, -1.2129e-01,  5.5456e-01, -8.1352e-01],\n",
      "         [-8.1735e-02, -1.5504e-01,  2.1724e-01, -1.1934e-01,  5.5539e-01,\n",
      "          -1.1719e+00,  6.9274e-01, -4.7354e-01, -1.7386e-01,  1.1231e+00,\n",
      "           2.5515e-01, -8.7249e-01, -1.3923e-01, -2.5886e-01, -1.6283e+00,\n",
      "          -5.6692e-01, -8.8303e-01,  8.2815e-02, -4.0044e-01,  1.0767e-01,\n",
      "           1.7682e-01,  1.0706e+00, -1.3344e-01, -2.3564e-01, -3.7675e-01,\n",
      "           2.8725e-01, -4.0967e-01, -1.6955e-01, -2.1687e-01, -8.9189e-01,\n",
      "          -4.4074e-01, -8.1089e-01, -1.1645e+00, -8.8425e-01,  4.9509e-01,\n",
      "           1.3207e-01, -1.1298e+00, -6.4166e-01,  1.2997e+00, -8.1562e-01,\n",
      "          -7.3276e-01, -7.3281e-01,  6.1884e-01, -1.3011e+00,  2.3096e-01,\n",
      "           2.8574e-01, -1.4970e-01, -2.0399e-01,  1.4192e+00, -6.0059e-02],\n",
      "         [ 1.0712e-01, -4.9892e-01,  4.1654e-01,  1.9627e-02, -4.7439e-01,\n",
      "          -4.4405e-01,  1.2217e-01, -8.1833e-01, -6.9642e-01,  2.8395e-02,\n",
      "           1.9117e-01,  5.0155e-02, -2.6403e-01, -9.9604e-01, -5.6063e-01,\n",
      "           3.2691e-01,  1.1982e-02, -8.8737e-01,  5.0483e-01,  2.3271e-02,\n",
      "           7.3111e-02,  5.5602e-02, -8.1120e-01,  5.5329e-01, -1.6665e-01,\n",
      "           5.6915e-01,  1.9587e-01,  3.2113e-02, -3.1710e-01, -4.7005e-01,\n",
      "          -2.9163e-01, -1.7819e-01, -2.1619e-01, -1.0792e+00,  5.4234e-01,\n",
      "          -6.8323e-01, -2.6718e-01, -2.0505e-01,  1.3623e+00,  2.0211e-01,\n",
      "          -1.4527e-01, -1.1034e+00,  8.9099e-01, -1.0392e+00, -5.1870e-01,\n",
      "           1.9737e-01, -8.8851e-02, -6.5839e-01,  2.0383e-01, -3.0169e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ]
}
